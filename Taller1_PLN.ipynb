{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoxchUmcy-dH"
      },
      "source": [
        "# Taller No. 1\n",
        "\n",
        "Objetivo. La idea es tratar de preprocesar el dataset **Europarl** (Large Corpus Spanish), tokenizarlo, limpiarlo, obtener el modelo de embedding usando Fasttext y comparar la similaridad con el modelo **Word2vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weGNrBIO2OFN"
      },
      "source": [
        "Integrantes:\n",
        "\n",
        "- Danilo Arvalo\n",
        "- William Ballesteros\n",
        "- Mavelyn Sterling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIa3CiQqW6H5"
      },
      "source": [
        "## Problema 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXjMfOxShQ9"
      },
      "source": [
        "### Punto 1.**texto en negrita** Traer el dataset de Huggin Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Po0GUzZSPZG"
      },
      "source": [
        " Acceder a Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3uIBaesSPAx",
        "outputId": "5aff17b0-31c4-4002-9e63-51e3b0fd73de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IW-QQAqTWlr",
        "outputId": "de092bc5-a172-4bd7-9348-f152d68ad65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 5 línas del archivo Europarl:\n",
            "----------------------------------------------\n",
            "Aprobación del acta\n",
            "\n",
            "El Acta de la sesión de ayer ha sido distribuida.\n",
            "\n",
            "¿Hay alguna observación?\n",
            "\n",
            "Señora Presidenta, con la mayor brevedad posible.\n",
            "\n",
            "Se trata, más que nada, del acta de anteayer.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ruta al archivo en Google Drive\n",
        "file_path = '/content/drive/MyDrive/Europarl.txt'\n",
        "\n",
        "print(\"Primeras 5 línas del archivo Europarl:\")\n",
        "print(\"----------------------------------------------\")\n",
        "\n",
        "# Abre el archivo y lee las primeras 5 líneas\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for _ in range(5):\n",
        "        line = file.readline()\n",
        "        print(line)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HgbKGNQWIpF"
      },
      "source": [
        "### Punto 2. Segmentar el texto en sentencias, se debe crear una lista de listas con cada sentencia y luego tokenizadas por palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPePFbBkbbt-",
        "outputId": "12e340fd-c3e5-44af-83dc-61b40076c174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZBPw2oabcml",
        "outputId": "7545324a-27aa-4149-c384-36b3bea49960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Descargar el modelo de tokenización para el idioma\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Ruta al archivo en Google Drive\n",
        "file_path = '/content/drive/MyDrive/Europarl.txt'\n",
        "\n",
        "# Lista de listas para almacenar las sentencias tokenizadas\n",
        "all_sentences_tokenized = []\n",
        "\n",
        "# Abre el archivo y lee el texto completo\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9c9vQgLk4Ja",
        "outputId": "554d7289-d5ad-40b4-c40c-98812ae4b94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencias tokenizadas:  2175949\n"
          ]
        }
      ],
      "source": [
        "# Segmenta el texto en sentencias\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Tokeniza cada sentencia en palabras\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    all_sentences_tokenized.append(tokens)\n",
        "\n",
        "print('sentencias tokenizadas: ',len(all_sentences_tokenized))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKqTVtCfxWmj",
        "outputId": "4b23a0ec-7a65-406a-a7d3-bd6dee6b20b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 10 sentencias tokenizadas:\n",
            "['Aprobación', 'del', 'acta', 'El', 'Acta', 'de', 'la', 'sesión', 'de', 'ayer', 'ha', 'sido', 'distribuida', '.']\n",
            "['¿Hay', 'alguna', 'observación', '?']\n",
            "['Señora', 'Presidenta', ',', 'con', 'la', 'mayor', 'brevedad', 'posible', '.']\n",
            "['Se', 'trata', ',', 'más', 'que', 'nada', ',', 'del', 'acta', 'de', 'anteayer', '.']\n",
            "['Ruego', 'comprensión', 'ya', 'que', 'para', 'no', 'molestar', 'a', 'nuestros', 'colegas', ',', 'el', 'Sr.', 'Pirker', 'y', 'yo', 'mismo', 'pasamos', 'una', 'nota', 'escrita', 'durante', 'la', 'votación', 'diciendo', 'que', 'nuestros', 'aparatos', 'de', 'voto', 'no', 'habían', 'funcionado', 'en', 'la', 'votación', 'sobre', 'la', 'conferencia', 'climática', '.']\n",
            "['Hice', 'saber', 'esta', 'circunstancia', '.']\n",
            "['Se', 'me', 'dijo', 'que', 'había', 'quedado', 'registrada', ',', 'pero', 'no', 'fue', 'así', '.']\n",
            "['Pido', 'que', 'esto', 'se', 'aclare', ',', 'ya', 'que', 'de', 'lo', 'contrario', 'no', 'podré', 'solucionar', 'en', 'el', 'futuro', 'tales', 'circunstancias', 'de', 'forma', 'escrita', ',', 'sino', 'que', 'deberé', 'manifestarme', ',', 'como', 'hacen', 'algunos', 'colegas', ',', 'de', 'forma', 'oral', '.']\n",
            "['Sin', 'embargo', 'esto', 'supone', 'un', 'gasto', 'de', 'tiempo', 'importante', '.']\n",
            "['Nos', 'ocuparemos', 'de', 'esta', 'rectificación', ',', 'señor', 'Posselt.', ')']\n"
          ]
        }
      ],
      "source": [
        "# Imprime solo las primeras 10 sentencias tokenizadas\n",
        "print(\"Primeras 10 sentencias tokenizadas:\")\n",
        "for i in range(10):\n",
        "    print(all_sentences_tokenized[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RRRGG5Cx2OX",
        "outputId": "e4545581-d146-4d5e-fd98-be9b48a5690f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El DataFrame se ha guardado como un archivo de texto (.txt) en: /content/drive/My Drive/Europarl_tokenized.txt\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Crear un DataFrame\n",
        "df = pd.DataFrame({'Sentences': all_sentences_tokenized})\n",
        "\n",
        "# Guardar el DataFrame como un archivo de texto (.txt)\n",
        "output_file_path = '/content/drive/My Drive/Europarl_tokenized.txt'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    for sentence_tokens in all_sentences_tokenized:\n",
        "        sentence_text = ' '.join(sentence_tokens)\n",
        "        output_file.write(sentence_text + '\\n')\n",
        "\n",
        "print(\"El DataFrame se ha guardado como un archivo de texto (.txt) en:\", output_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Ca7XsoYmy3"
      },
      "source": [
        "### Punto 3. Limpiar cada una de las sentencias tokenizadas usando el criterio definido en el punto 1.2. (Limpieza de datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrfTDPjLZzJa"
      },
      "source": [
        "Limpieza de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcse0KGQaUrv",
        "outputId": "52fde2d7-baf7-468a-9be3-d351cc851280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vre0OnMRxHk5"
      },
      "outputs": [],
      "source": [
        "# Expresiones regulares y funciones de limpieza\n",
        "def limpiar_sentencia(tokens):\n",
        "    # Eliminar números\n",
        "    new_tokens = [re.sub(r'\\d', '', token) for token in tokens]\n",
        "\n",
        "    # Eliminar stopwords\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    new_tokens = [token for token in new_tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Eliminar puntuación y otros símbolos especiales, incluyendo »\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation + '¿¡»'))\n",
        "    new_tokens = [re_punc.sub('', token) for token in new_tokens]\n",
        "\n",
        "    # Otras eliminaciones específicas\n",
        "    new_tokens = [re.sub(r\"\\\\i\", \"\", token) for token in new_tokens]\n",
        "\n",
        "    # Eliminar tokens vacíos después de la limpieza\n",
        "    new_tokens = [token for token in new_tokens if token.strip()]\n",
        "\n",
        "    return new_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGB9u4mOzURT",
        "outputId": "0cc18b5f-00e9-48f7-cd61-a23b5e4d1ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de palabras a limpiar:  10000\n"
          ]
        }
      ],
      "source": [
        "# Ruta al archivo en Google Drive\n",
        "file_path = '/content/drive/My Drive/Europarl.txt'\n",
        "\n",
        "# Lista de listas para almacenar las sentencias tokenizadas y limpias\n",
        "all_sentences_cleaned = []\n",
        "\n",
        "# Abre el archivo y lee el texto completo\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokeniza cada sentencia en palabras y realiza la limpieza\n",
        "def limpiezaTokens(sentences):\n",
        "  for index, sentence in enumerate(sentences):\n",
        "      if index < 10000: # Aqui puede modificar el numero de oraciones de las cuales quiere limpiar\n",
        "          #print(sentence)\n",
        "          tokens = word_tokenize(sentence)\n",
        "          cleaned_tokens = limpiar_sentencia(tokens)\n",
        "          all_sentences_cleaned.append(cleaned_tokens)\n",
        "      else:\n",
        "          break\n",
        "  print('Numero de palabras a limpiar: ',len(all_sentences_cleaned))\n",
        "\n",
        "limpiezaTokens(sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jdS1JMF9y1E",
        "outputId": "a3eb37f4-466b-41b7-fb41-c9c4ae97e75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Algunas de las listas de sentencias solo con palabras (limpias):\n",
            "['Aprobación', 'acta', 'Acta', 'sesión', 'ayer', 'sido', 'distribuida']\n",
            "['Hay', 'alguna', 'observación']\n",
            "['Señora', 'Presidenta', 'mayor', 'brevedad', 'posible']\n",
            "['trata', 'acta', 'anteayer']\n",
            "['Ruego', 'comprensión', 'molestar', 'colegas', 'Sr', 'Pirker', 'mismo', 'pasamos', 'nota', 'escrita', 'votación', 'diciendo', 'aparatos', 'voto', 'funcionado', 'votación', 'conferencia', 'climática']\n",
            "['Hice', 'saber', 'circunstancia']\n",
            "['dijo', 'quedado', 'registrada', 'así']\n",
            "['Pido', 'aclare', 'contrario', 'podré', 'solucionar', 'futuro', 'tales', 'circunstancias', 'forma', 'escrita', 'sino', 'deberé', 'manifestarme', 'hacen', 'colegas', 'forma', 'oral']\n",
            "['embargo', 'supone', 'gasto', 'tiempo', 'importante']\n",
            "['ocuparemos', 'rectificación', 'señor', 'Posselt']\n",
            "['Señora', 'Presidenta', 'sino', 'cierta', 'tristeza', 'pongo', 'pie', 'mañana', 'hacer', 'comentario', 'acta', 'ayer']\n",
            "['Ayer', 'intervine', 'referirme', 'inexactitud', 'parecer', 'encontrado', 'colega', 'Sr', 'Macartney', 'relativa', 'error', 'según', 'dice', 'traducción']\n",
            "['Ayer', 'afirmaba', 'versión', 'original', 'enmienda', 'puesto', 'palabras', 'English', 'Beef', 'sido', 'traducidas', 'Bristish', 'Beef']\n",
            "['lado', 'Asamblea', 'visto', 'ahora', 'texto', 'original', 'presentado', 'grupo']\n",
            "['Decía', 'claramente', 'Bristish', 'Beef']\n"
          ]
        }
      ],
      "source": [
        "# Crear un DataFrame con las sentencias tokenizadas y limpias\n",
        "df_cleaned = pd.DataFrame({'Sentences': all_sentences_cleaned})\n",
        "\n",
        "# Imprimir algunas de las listas de sentencias solo con palabras (limpias)\n",
        "print(\"Algunas de las listas de sentencias solo con palabras (limpias):\")\n",
        "for i in range(15):\n",
        "    print(df_cleaned['Sentences'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE5u9eruYTrv"
      },
      "source": [
        "### Punto 4. Definir las características de entrenamiento y creación (num_features, sg, context_size, etc) para el uso de FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94MRmbSklZy8",
        "outputId": "b91a1421-2340-44c1-b2e8-df5f97a22735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install cython\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdSTBPHStdap",
        "outputId": "37a67747-3eb2-46fd-9cc9-641e5d1e0183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Instalar la librería Gensim si no está instalada\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tum8vG_n7815"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import multiprocessing\n",
        "\n",
        "#Sentencias tokenizadas que el modelo FastText usa:\n",
        "sentences = all_sentences_tokenized\n",
        "# Configurar las características del modelo\n",
        "num_features = 300  # Dimensionalidad del embeddings\n",
        "min_word_count = 1  # Umbral mínimo de recuento de palabras\n",
        "num_workers = multiprocessing.cpu_count()  # Numero de subprocesos que se ejecutan en paralelo\n",
        "context_size = 5  # Longitud de la ventana de contexto\n",
        "seed = 1\n",
        "\n",
        "# Definir las características del modelo FastText\n",
        "fasttext_model = FastText(\n",
        "    sentences=sentences[0:10000], #Numero de oraciones que usa el modelo\n",
        "    vector_size=num_features,\n",
        "    window=context_size,\n",
        "    min_count=min_word_count,\n",
        "    workers=num_workers,\n",
        "    sg=1  # Usar el algoritmo skip-gram\n",
        ")\n",
        "\n",
        "# Guardar el modelo en formato Word2Vec (texto)\n",
        "modelFasttext= FastText(vector_size=300, window=5, min_count=1)\n",
        "modelFasttext.build_vocab(corpus_iterable=sentences[1:300])\n",
        "modelFasttext.save(\"word2vec.model\")\n",
        "modelFasttext.wv.save_word2vec_format('fasttext_skip_model'+str(300)+ '.txt', binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3HapE9KW2b4"
      },
      "source": [
        "### Punto 5: Se debe generar el archivo vectorizado de todas las palabras del texto del dataset Europarl (Large Corpus Spanish) con FastText . Para probar la similaridad de las palabras con respecto a los modelos de Word2vec y FastText se deden probar mínimo tres palabras del vocabulario del dataset y comparar la similaridad del archivo generado con respecto al modelo de Word2vec que puede ser generado con las mismas sentencias de Europarl (Large Corpus Spanish).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaINM5LDyKz3"
      },
      "outputs": [],
      "source": [
        "#vectorizacion con Word2Vec\n",
        "\n",
        "modelWord2vec= Word2Vec(sentences=sentences[0:10000], vector_size= num_features, window=5, min_count=min_word_count, workers=4)\n",
        "modelWord2vec.save(\"word2vec.model\")\n",
        "modelWord2vec.wv.save_word2vec_format('word2vec_skip'+str(300)+'.txt',binary=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVlM7NCc8E1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cf8151-c072-40e2-f0a3-87732509fe4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Señor', 0.9945002794265747),\n",
              " ('Presidenta', 0.9933640360832214),\n",
              " ('Bush', 0.9898671507835388),\n",
              " ('Šefčovič', 0.9896451234817505),\n",
              " ('señoras', 0.9893759489059448),\n",
              " ('Dae-Jung', 0.9874404072761536),\n",
              " ('Schulz', 0.987196683883667),\n",
              " ('Putin', 0.9865538477897644),\n",
              " ('permítame', 0.9846318364143372),\n",
              " ('–', 0.9844339489936829)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#Similitudes\n",
        "##comparacion 1 con el modelo World2Vec\n",
        "modelWord2vec.wv.most_similar('Señora')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSuU19NB8OFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af34736f-1729-404f-8118-56f571be0020"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Señor', 0.5965688228607178),\n",
              " ('señora', 0.49922695755958557),\n",
              " ('supra', 0.2622785270214081),\n",
              " ('Se', 0.19818784296512604),\n",
              " ('débil', 0.19816645979881287),\n",
              " ('ahora', 0.19454160332679749),\n",
              " ('Ahora', 0.19263464212417603),\n",
              " ('le', 0.18629927933216095),\n",
              " ('inconveniente', 0.17312182486057281),\n",
              " ('Éste', 0.16349492967128754)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#Similitudes\n",
        "##comparacion 1 con el modelo FASTTEXT\n",
        "modelFasttext.wv.most_similar('Señora')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Similitudes\n",
        "##comparacion 2 con el modelo World2Vec\n",
        "modelWord2vec.wv.most_similar('vegetariano')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-s-BGqutQBr",
        "outputId": "ccd5645a-6bd5-49cd-930e-53b3f0b253c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('yo', 0.954031229019165),\n",
              " ('decirles', 0.9532796144485474),\n",
              " ('pedirle', 0.9523018002510071),\n",
              " ('imposible', 0.951831579208374),\n",
              " ('preguntarle', 0.9516990780830383),\n",
              " ('acabado', 0.9509478211402893),\n",
              " ('insisto', 0.9499616026878357),\n",
              " ('quisiera', 0.9497095346450806),\n",
              " ('voy', 0.9492710828781128),\n",
              " ('Puedo', 0.949180543422699)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiK6yu1K8Otl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b68ff6-f696-48e8-ae8c-88d6a7208641"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('recepción', 0.1739891767501831),\n",
              " ('mecanismo', 0.15601719915866852),\n",
              " ('intención', 0.1479145884513855),\n",
              " ('Unitaria', 0.147610604763031),\n",
              " ('informe', 0.14682605862617493),\n",
              " ('aprobación', 0.1461014300584793),\n",
              " ('Reformistas', 0.14599668979644775),\n",
              " ('etnicismo', 0.13971993327140808),\n",
              " ('suerte', 0.13823676109313965),\n",
              " ('-B4-0903/97', 0.1370798796415329)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#Similitudes\n",
        "##comparacion 2 con el modelo FASTTEXT\n",
        "modelFasttext.wv.most_similar('vegetariano')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWA6Gicp8QYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c3a14b-cfe5-4dde-871e-6a23012d2efb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('totalmente', 0.9979119300842285),\n",
              " ('Túnez', 0.9973028898239136),\n",
              " ('elevado', 0.9970646500587463),\n",
              " ('estado', 0.9968712329864502),\n",
              " ('ocasiones', 0.996783435344696),\n",
              " ('inquietud', 0.9967804551124573),\n",
              " ('referencia', 0.9965175986289978),\n",
              " ('dicha', 0.9965155124664307),\n",
              " ('hubo', 0.9965002536773682),\n",
              " ('directo', 0.9962692260742188)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Similitudes\n",
        "##comparacion 3 con el modelo World2Vec\n",
        "modelWord2vec.wv.most_similar('pasar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3rD8rbe8RIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587a567d-2d4a-4265-d132-166ff25e3bd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pasara', 0.5819039344787598),\n",
              " ('pasado', 0.39989519119262695),\n",
              " ('pasamos', 0.3411489725112915),\n",
              " ('paso', 0.24304892122745514),\n",
              " ('par', 0.22995376586914062),\n",
              " ('oral', 0.2238299548625946),\n",
              " ('pensar', 0.20620514452457428),\n",
              " ('estar', 0.1820765882730484),\n",
              " ('fomentar', 0.18002858757972717),\n",
              " ('precisar', 0.17427794635295868)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#Similitudes\n",
        "##comparacion 3 con el modelo FASTTEXT\n",
        "modelFasttext.wv.most_similar('pasar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-KimTyG4nsP"
      },
      "outputs": [],
      "source": [
        "##Modelo FastText en txt:\n",
        "\n",
        "for t in [num_features]:\n",
        "  modelFasttext = FastText(\n",
        "      sentences= sentences[0:10000],\n",
        "      vector_size=t,\n",
        "      window=5,\n",
        "      min_count=min_word_count,\n",
        "      workers=num_workers\n",
        "  )\n",
        "  modelFasttext.wv.save_word2vec_format('fasttext_skip_model'+str(t)+'.txt',binary=False)\n",
        "  del modelFasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWBLIkuW68OY"
      },
      "outputs": [],
      "source": [
        "##Modelo Word2vc en txt:\n",
        "\n",
        "for t in [num_features]:\n",
        "  modelWord2vec = Word2Vec(\n",
        "      sentences= sentences[0:10000],\n",
        "      vector_size=t,\n",
        "      window=8,\n",
        "      min_count=min_word_count,\n",
        "      workers=num_workers\n",
        "  )\n",
        "  modelWord2vec.wv.save_word2vec_format('word2vec_skip'+str(t) + '.txt',binary=False)\n",
        "  del modelWord2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fENQXtfpycOJ"
      },
      "source": [
        "## Problema 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXc1Lr-Vy7GM"
      },
      "outputs": [],
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Texto de entrada\n",
        "texto = \"Muchos años después, frente al pelotón de fusilamiento, el coronel Aureliano Buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo. Macondo era entonces unaaldeadeveintecasasdebarroycañabravaconstruidasalaorilladeun río de aguas diáfanas que se precipitaban por un lecho de piedras pulidas, blancas y enormes como huevos prehistóricos.\"\n",
        "\n",
        "# Preprocesamiento del texto\n",
        "texto = re.sub(r'\\s+', ' ', texto)  # Eliminar espacios adicionales\n",
        "oraciones = [re.findall(r'\\b\\w+\\b', oracion.lower()) for oracion in texto.split('.')]  # Dividir en oraciones y tokenizar\n",
        "\n",
        "# Entrenar el modelo Word2Vec\n",
        "modelo_dim_30 = Word2Vec(sentences=oraciones, vector_size=30, window=3, sg=1, min_count=1)\n",
        "modelo_dim_50 = Word2Vec(sentences=oraciones, vector_size=50, window=3, sg=1, min_count=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ghGtlnI0nu5"
      },
      "source": [
        "### Imprimir la salida de los vectores de W1+b1 para los dos embeddings (30 y 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI6lx48s0kao"
      },
      "outputs": [],
      "source": [
        "# Función para imprimir los vectores W1 + b1\n",
        "def imprimir_vectores_w1_b1(modelo, dimension):\n",
        "    print(f\"Vectores de W1 + b1 (Dimensión {dimension}):\")\n",
        "    for palabra in modelo.wv.index_to_key:\n",
        "        indice = modelo.wv.key_to_index[palabra]\n",
        "        vector_w1_b1 = modelo.wv[palabra] + modelo.wv.get_normed_vectors()[indice]\n",
        "        print(f\"{palabra}: {vector_w1_b1}\")\n",
        "\n",
        "# Imprimir vectores para ambas dimensiones\n",
        "imprimir_vectores_w1_b1(modelo_dim_30, dimension=30)\n",
        "imprimir_vectores_w1_b1(modelo_dim_50, dimension=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trqbsJp_0QgR"
      },
      "source": [
        "### Visualización en tsne de los vectores del texto con los embeddings de dimensión de 30 y 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWus2_ywzSYG"
      },
      "outputs": [],
      "source": [
        "# Función para visualizar palabras usando t-SNE\n",
        "def visualizar_tsne(modelo, dimension):\n",
        "    words = list(modelo.wv.index_to_key)\n",
        "    vectors_tsne = TSNE(n_components=2, random_state=42).fit_transform(modelo.wv.vectors)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1], edgecolors='k', c='r')\n",
        "    for word, (x, y) in zip(words, vectors_tsne):\n",
        "        plt.text(x, y, word)\n",
        "\n",
        "    plt.title(f'Visualización t-SNE (Dimensión {dimension})')\n",
        "    plt.show()\n",
        "\n",
        "# Visualizar palabras en el plano cartesiano para ambas dimensiones\n",
        "print(\"             \")\n",
        "print(\"    Visualización con t-SNE (Dim 30)         \")\n",
        "print(\"             \")\n",
        "visualizar_tsne(modelo_dim_30, dimension=30)\n",
        "print(\"             \")\n",
        "print(\"             \")\n",
        "print(\"    Visualización con t-SNE (Dim 50)         \")\n",
        "print(\"             \")\n",
        "visualizar_tsne(modelo_dim_50, dimension=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g60_CnmS0Ggj"
      },
      "source": [
        "### Distancia euclidiana entre 3 palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaYqr9woz1Tw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def euclidean_dist(vec1, vec2):\n",
        "    return np.linalg.norm(vec1 - vec2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RVjS9jy0EU2"
      },
      "outputs": [],
      "source": [
        "# Palabras de interés\n",
        "palabra1 = 'coronel'\n",
        "palabra2 = 'padre'\n",
        "palabra3 = 'hielo'\n",
        "\n",
        "# Obtener vectores W1 + b1 para las palabras de interés\n",
        "vector1 = modelo_dim_30.wv[palabra1] + modelo_dim_30.wv.get_normed_vectors()[modelo_dim_30.wv.key_to_index[palabra1]]\n",
        "vector2 = modelo_dim_30.wv[palabra2] + modelo_dim_30.wv.get_normed_vectors()[modelo_dim_30.wv.key_to_index[palabra2]]\n",
        "vector3 = modelo_dim_30.wv[palabra3] + modelo_dim_30.wv.get_normed_vectors()[modelo_dim_30.wv.key_to_index[palabra3]]\n",
        "\n",
        "# Calcular distancias euclidianas\n",
        "distancia_12 = euclidean_dist(vector1, vector2)\n",
        "distancia_13 = euclidean_dist(vector1, vector3)\n",
        "distancia_23 = euclidean_dist(vector2, vector3)\n",
        "\n",
        "# Imprimir resultados\n",
        "print(f'Distancia entre \"{palabra1}\" y \"{palabra2}\": {distancia_12}')\n",
        "print(f'Distancia entre \"{palabra1}\" y \"{palabra3}\": {distancia_13}')\n",
        "print(f'Distancia entre \"{palabra2}\" y \"{palabra3}\": {distancia_23}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}